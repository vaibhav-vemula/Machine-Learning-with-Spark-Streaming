{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Message</th>\n",
       "      <th>Spam/Ham</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transfers from ees</td>\n",
       "      <td>attached is the latest version of the cost cen...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fw : re ivanhoe e . s . d</td>\n",
       "      <td>fyi , kim .\\n- - - - - original message - - - ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>re : enerfin meter 980439 for 10 / 00</td>\n",
       "      <td>it did but tetco prorated the flow between the...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>meoh plant status</td>\n",
       "      <td>the methanol plant has determined extensive re...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>re : tenaska iv</td>\n",
       "      <td>i tried calling you this am but your phone rol...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10339</th>\n",
       "      <td>transwestern open season</td>\n",
       "      <td>( see attached file : twopenseason . doc )\\n- ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10340</th>\n",
       "      <td>valero 8018 and 1394</td>\n",
       "      <td>gary ,\\nwhat is the status of this ?\\nhc\\n- - ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341</th>\n",
       "      <td>tenaska iv 4 / 01</td>\n",
       "      <td>we need to change the demand fee on deal 38425...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10342</th>\n",
       "      <td>re : releases</td>\n",
       "      <td>louise ,\\nthanks so much for your speedy reply...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10343</th>\n",
       "      <td>ranks communication</td>\n",
       "      <td>after getting your imput , i think this is the...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10344 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Subject  \\\n",
       "0                         transfers from ees   \n",
       "1                  fw : re ivanhoe e . s . d   \n",
       "2      re : enerfin meter 980439 for 10 / 00   \n",
       "3                          meoh plant status   \n",
       "4                            re : tenaska iv   \n",
       "...                                      ...   \n",
       "10339               transwestern open season   \n",
       "10340                   valero 8018 and 1394   \n",
       "10341                      tenaska iv 4 / 01   \n",
       "10342                          re : releases   \n",
       "10343                    ranks communication   \n",
       "\n",
       "                                                 Message Spam/Ham  \n",
       "0      attached is the latest version of the cost cen...     spam  \n",
       "1      fyi , kim .\\n- - - - - original message - - - ...     spam  \n",
       "2      it did but tetco prorated the flow between the...      ham  \n",
       "3      the methanol plant has determined extensive re...      ham  \n",
       "4      i tried calling you this am but your phone rol...     spam  \n",
       "...                                                  ...      ...  \n",
       "10339  ( see attached file : twopenseason . doc )\\n- ...      ham  \n",
       "10340  gary ,\\nwhat is the status of this ?\\nhc\\n- - ...      ham  \n",
       "10341  we need to change the demand fee on deal 38425...      ham  \n",
       "10342  louise ,\\nthanks so much for your speedy reply...     spam  \n",
       "10343  after getting your imput , i think this is the...      ham  \n",
       "\n",
       "[10344 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../dataset/spam/train.csv',index_col=False)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.drop(df.tail(20000).index, inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "js = df.to_json(orient ='records')\n",
    "f = open('ddd.json','w')\n",
    "f.write(js)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "x = spark.read.json('ddd.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+\n",
      "|             Message|Spam/Ham|             Subject|\n",
      "+--------------------+--------+--------------------+\n",
      "|attached is the l...|    spam|  transfers from ees|\n",
      "|fyi , kim .\\n- - ...|    spam|fw : re ivanhoe e...|\n",
      "|it did but tetco ...|     ham|re : enerfin mete...|\n",
      "|the methanol plan...|     ham|   meoh plant status|\n",
      "|i tried calling y...|    spam|     re : tenaska iv|\n",
      "|fyi , kim .\\n- - ...|    spam|fw : re ivanhoe e...|\n",
      "|hi ,\\ni am forwar...|    spam|fw : memo : re : ...|\n",
      "|enron replaces fa...|     ham|      enron mentions|\n",
      "|attached is the l...|    spam|  transfers from ees|\n",
      "|start date : 2 / ...|    spam|start date : 2 / ...|\n",
      "|start date : 2 / ...|    spam|start date : 2 / ...|\n",
      "|start date : 12 /...|     ham|start date : 12 /...|\n",
      "|fyi , kim .\\n- - ...|    spam|fw : re ivanhoe e...|\n",
      "|this is a complet...|     ham|re : priority cus...|\n",
      "|enron tiger team ...|     ham|new consultants a...|\n",
      "|business highligh...|     ham|  entouch newsletter|\n",
      "|attached is the w...|     ham|  weekly deal report|\n",
      "|attached is the l...|    spam|  transfers from ees|\n",
      "|attached is the l...|    spam|  transfers from ees|\n",
      "|start date : 2 / ...|    spam|start date : 2 / ...|\n",
      "+--------------------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+\n",
      "|                text|class|length|\n",
      "+--------------------+-----+------+\n",
      "|attached is the l...| spam|   868|\n",
      "|fyikim . original...| spam|  1180|\n",
      "|it did but tetco ...|  ham|  4512|\n",
      "|the methanol plan...|  ham|   360|\n",
      "|i tried calling y...| spam|  2284|\n",
      "|fyikim . original...| spam|  1180|\n",
      "|hi i am forwardin...| spam|  1674|\n",
      "|attached is the l...| spam|   868|\n",
      "|start date2602  h...| spam|   632|\n",
      "|start date2602  h...| spam|   632|\n",
      "|start date123001 ...|  ham|   536|\n",
      "|fyikim . original...| spam|  1180|\n",
      "|this is a complet...|  ham|  1937|\n",
      "|enron tiger team ...|  ham|   553|\n",
      "|business highligh...|  ham|  8074|\n",
      "|attached is the w...|  ham|   140|\n",
      "|attached is the l...| spam|   868|\n",
      "|attached is the l...| spam|   868|\n",
      "|start date2602  h...| spam|   632|\n",
      "|start date2602  h...| spam|   632|\n",
      "+--------------------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length, regexp_replace\n",
    "\n",
    "data = x.withColumn('length',length(x['Message'])).withColumnRenamed(\"Spam/Ham\",\"class\").withColumnRenamed(\"Message\",\"text\")\n",
    "data = data.drop('Subject')\n",
    "data = data.withColumn(\"text\", regexp_replace(\"text\", \",\", \"\"))\\\n",
    "    .withColumn(\"text\", regexp_replace(\"text\", \",\", \"\"))\\\n",
    "    .withColumn(\"text\", regexp_replace(\"text\", \":\", \"\"))\\\n",
    "    .withColumn(\"text\", regexp_replace(\"text\", \"-\", \"\"))\\\n",
    "    .withColumn(\"text\", regexp_replace(\"text\", \"/\", \"\"))\\\n",
    "    .withColumn(\"text\", regexp_replace(\"text\", \"\\n\", \"\"))\\\n",
    "    .withColumn(\"text\", regexp_replace(\"text\", \"  \", \"\"))\\\n",
    "    .withColumn(\"text\", regexp_replace(\"text\", \";\", \"\"))\n",
    "data= data.where(data['length']<20000)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|class|       avg(length)|\n",
      "+-----+------------------+\n",
      "|  ham|1295.9862741197533|\n",
      "| spam|1566.7767653758542|\n",
      "+-----+------------------+\n",
      "\n",
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      " |-- length: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupby('class').mean().show()\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer,StopWordsRemover, CountVectorizer,IDF,StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n",
    "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "count_vec = CountVectorizer(inputCol='stop_tokens',outputCol='c_vec')\n",
    "idf = IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")\n",
    "ham_spam_to_num = StringIndexer(inputCol='class',outputCol='label')\n",
    "clean_up = VectorAssembler(inputCols=['tf_idf','length'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "# Use defaults\n",
    "nb = NaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "data_prep_pipe = Pipeline(stages=[ham_spam_to_num,tokenizer,stopremove,count_vec,idf,clean_up])\n",
    "cleaner = data_prep_pipe.fit(data)\n",
    "clean_data = cleaner.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(101720,[0,1,19,2...|\n",
      "|  0.0|(101720,[0,2,4,5,...|\n",
      "|  1.0|(101720,[0,1,4,5,...|\n",
      "|  1.0|(101720,[0,79,91,...|\n",
      "|  0.0|(101720,[0,1,5,6,...|\n",
      "|  0.0|(101720,[0,2,4,5,...|\n",
      "|  0.0|(101720,[0,4,5,6,...|\n",
      "|  0.0|(101720,[0,1,19,2...|\n",
      "|  0.0|(101720,[0,1,2,3,...|\n",
      "|  0.0|(101720,[0,1,2,3,...|\n",
      "|  1.0|(101720,[0,2,3,14...|\n",
      "|  0.0|(101720,[0,2,4,5,...|\n",
      "|  1.0|(101720,[0,1,2,4,...|\n",
      "|  1.0|(101720,[0,4,6,8,...|\n",
      "|  1.0|(101720,[0,1,5,6,...|\n",
      "|  1.0|(101720,[35,255,2...|\n",
      "|  0.0|(101720,[0,1,19,2...|\n",
      "|  0.0|(101720,[0,1,19,2...|\n",
      "|  0.0|(101720,[0,1,2,3,...|\n",
      "|  0.0|(101720,[0,1,2,3,...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_data = clean_data.select(['label','features'])\n",
    "clean_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(101720,[0,1,2,3,...|[-1286.7920745740...|[1.0,6.7080016980...|       0.0|\n",
      "|  0.0|(101720,[0,1,2,3,...|[-1286.7920745740...|[1.0,6.7080016980...|       0.0|\n",
      "|  0.0|(101720,[0,1,2,3,...|[-1286.7920745740...|[1.0,6.7080016980...|       0.0|\n",
      "|  0.0|(101720,[0,1,2,3,...|[-1286.7920745740...|[1.0,6.7080016980...|       0.0|\n",
      "|  0.0|(101720,[0,1,2,3,...|[-1286.7920745740...|[1.0,6.7080016980...|       0.0|\n",
      "|  0.0|(101720,[0,1,2,3,...|[-1286.7920745740...|[1.0,6.7080016980...|       0.0|\n",
      "|  0.0|(101720,[0,1,2,3,...|[-1286.7920745740...|[1.0,6.7080016980...|       0.0|\n",
      "|  0.0|(101720,[0,1,2,3,...|[-1286.7920745740...|[1.0,6.7080016980...|       0.0|\n",
      "|  0.0|(101720,[0,1,2,3,...|[-1286.7920745740...|[1.0,6.7080016980...|       0.0|\n",
      "|  0.0|(101720,[0,1,2,3,...|[-1286.7920745740...|[1.0,6.7080016980...|       0.0|\n",
      "|  0.0|(101720,[0,1,2,3,...|[-1286.7920745740...|[1.0,6.7080016980...|       0.0|\n",
      "|  0.0|(101720,[0,1,2,3,...|[-1286.7920745740...|[1.0,6.7080016980...|       0.0|\n",
      "|  0.0|(101720,[0,1,2,3,...|[-1286.7920745740...|[1.0,6.7080016980...|       0.0|\n",
      "|  0.0|(101720,[0,1,2,3,...|[-1286.7920745740...|[1.0,6.7080016980...|       0.0|\n",
      "|  0.0|(101720,[0,1,2,3,...|[-1286.7920745740...|[1.0,6.7080016980...|       0.0|\n",
      "|  0.0|(101720,[0,1,2,3,...|[-1286.7920745740...|[1.0,6.7080016980...|       0.0|\n",
      "|  0.0|(101720,[0,1,2,3,...|[-1286.7920745740...|[1.0,6.7080016980...|       0.0|\n",
      "|  0.0|(101720,[0,1,2,3,...|[-1286.7920745740...|[1.0,6.7080016980...|       0.0|\n",
      "|  0.0|(101720,[0,1,2,3,...|[-1286.7920745740...|[1.0,6.7080016980...|       0.0|\n",
      "|  0.0|(101720,[0,1,2,3,...|[-1286.7920745740...|[1.0,6.7080016980...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(training,testing) = clean_data.randomSplit([0.7,0.3])\n",
    "spam_predictor = nb.fit(training)\n",
    "test_results = spam_predictor.transform(testing)\n",
    "test_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting spam was: 0.9854758608382852\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(\"Accuracy of model at predicting spam was: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: (101720,[58],[-0.027514528651162422])\n",
      "Intercept: 0.5020358167192102\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(featuresCol = 'features', labelCol='label', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(training)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.491239\n",
      "r2: 0.034520\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|              label|\n",
      "+-------+-------------------+\n",
      "|  count|               7263|\n",
      "|   mean|0.49249621368580476|\n",
      "| stddev| 0.4999781107362106|\n",
      "|    min|                0.0|\n",
      "|    max|                1.0|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans().setK(4).setSeed(1)\n",
    "kmodel = kmeans.fit(training)\n",
    "predictions=kmodel.transform(testing)\n",
    "predictions = predictions.withColumn(\"prediction\", predictions[\"prediction\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2889739696076238"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluatorsvm = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluatorsvm.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
